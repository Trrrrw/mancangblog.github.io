[{"content":"从零理解 Transformer：为什么“注意力”改变了AI？ 人类如何处理信息？—— 从直觉理解“注意力” 假设你读这句话：“那只猫坐在垫子上，因为它很柔软。”\n你会自然关注：\n“它” 指代的是 “垫子”（而不是猫） “柔软” 是垫子的属性 关键直觉：人类在处理信息时，会动态关注不同部分的关联性。这正是 Transformer 的核心思想！\n传统模型的缺陷：RNN 为什么不够好？ RNN 的工作原理 传统模型（如RNN）像一条“传送带”，逐个单词处理： 句子：The → cat → sat → on → the → mat 处理：RNN逐步更新隐藏状态，最后输出结果\n问题：\n处理到句尾时，可能已经忘记句首的信息 无法并行计算（必须按顺序处理） 长距离依赖问题 例子：翻译 “The animal didn\u0026rsquo;t cross the street because it was too tired”\nRNN 可能混淆 “it” 指代的是 “animal” 还是 “street” 而人类能直接通过注意力找到关联 Transformer 的核心创新：自注意力机制 自注意力如何工作？ 想象你在阅读句子**\u0026ldquo;猫坐在垫子上\u0026rdquo;**时，大脑会本能地：\n1️⃣ 提取核心对象（\u0026ldquo;猫\u0026quot;是主体）\n2️⃣ 关联动作描述（\u0026ldquo;坐\u0026quot;定义状态）\n3️⃣ 忽略次要成分（\u0026ldquo;上\u0026quot;仅表方位）\n🛠️ 自注意力工作流程分解（以\u0026quot;猫\u0026quot;为例） 步骤1：构建\u0026quot;问题-答案\u0026quot;映射系统\n每个词语生成三组向量：\n向量类型 类比意义 实例说明 Query \u0026ldquo;我想知道什么？\u0026rdquo; \u0026ldquo;猫\u0026quot;需要知道自己的状态特征 Key \u0026ldquo;我能回答什么？\u0026rdquo; \u0026ldquo;坐\u0026quot;能提供动作信息 Value \u0026ldquo;我的完整语义是什么？\u0026rdquo; \u0026ldquo;垫子\u0026quot;包含物体属性信息 步骤2：建立语义关联度矩阵\n通过点积计算\u0026quot;猫\u0026quot;与其他词语的关联强度： $$ \\text{相似度}(Q_{\\text{猫}}, K_{\\text{坐}}) = \\frac{Q_{\\text{猫}} \\cdot K_{\\text{坐}}^T}{\\sqrt{d_k}} $$注意力权重分布\n步骤3：信息融合与特征增强\n加权聚合各词语的Value信息：\n$$ \\text{新表示}_{猫} = 0.6 \\times V_{猫} + 0.3 \\times V_{坐} + 0.1 \\times V_{垫子} $$ 此时\u0026quot;猫\u0026quot;的表示包含：\n自身物种特征（60%）\n坐姿状态信息（30%）\n所处环境线索（10%）\n为什么要“多头”注意力？—— 分工合作 头1：关注“猫”和“坐”的动作关系 头2：关注“垫子”和“上”的位置关系 头3：捕捉其他语法特征\n这样，每个头都能聚焦不同的信息，提高模型的表达能力。 如何实现“多头”注意力？—— 矩阵运算 矩阵乘法：计算注意力权重 点积：衡量相似度 Softmax：归一化权重 Transformer 的完整架构 两大核心模块 编码器（理解输入） 解码器（生成输出）\r↓ ↓\r[自注意力 + 前馈网络] [自注意力 + 交叉注意力 + 前馈网络]\r编码器详解 输入嵌入：将单词转换为向量 例：“猫” → [0.2, 1.3, -0.5, \u0026hellip;] 位置编码：添加位置信息（否则模型不知道单词顺序） 类似给每个词加“页码”：位置1 → 正弦波，位置2 → 不同正弦波\u0026hellip; 自注意力层：计算词与词之间的关系 前馈网络：对每个词独立做非线性变换 解码器如何工作？ 输入：已生成的部分结果（例如“猫 坐在”） 掩码自注意力：防止偷看未来信息（只能看已生成的词） 交叉注意力：连接编码器的输出（知道原文信息） 预测下一个词：输出概率分布（例如“垫子”的概率最高） 为什么 Transformer 如此强大？ 三大优势 优势 说明 类比 并行计算 同时处理所有单词，速度极快 多人协同工作 全局视野 任意两个词可直接交互，无论距离多远 直接查阅全书任意章节 灵活扩展 通过堆叠更多层提升性能 增加专家团队规模 实际影响 BERT：理解语言含义的模型（用于搜索引擎、问答系统） GPT：生成文本的模型（用于写作助手、聊天机器人） DALL·E：生成图像的模型也借鉴了注意力机制 关键数学公式 自注意力公式 $$\r\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\dfrac{QK^T}{\\sqrt{d_k}}\\right)V\r$$ Q（查询）：当前关注的焦点 K（键）：其他词的身份标识 V（值）：实际携带的信息 d_k：缩放因子，防止数值过大 位置编码公式 $$\rPE_{(pos,2i)} = \\sin\\left(pos/10000^{2i/d}\\right) \\\\ PE_{(pos,2i+1)} = \\cos\\left(pos/10000^{2i/d}\\right)\r$$ 通过正弦波组合，让模型感知位置差异 多头注意力公式 $$\r\\text{MultiHead}(Q, K, V) = \\text{Concat}\\left(\\text{head}_1, \\dots, \\text{head}_h\\right)W^O\r$$ 多个头并行计算，最后合并结果 前馈网络公式 $$\r\\text{FFN}(x) = \\text{max}(0, xW_1 + b_1)W_2 + b_2\r$$ 两层线性变换，增加非线性表达能力 损失函数公式 $$\r\\text{Loss} = -\\sum_{i=1}^{N}\\log p(y_i|x)\r$$ 交叉熵损失，用于训练模型 梯度下降公式 $$\r\\theta_{t+1} = \\theta_t - \\alpha \\nabla_{\\theta} \\text{Loss}\r$$ 反向传播更新模型参数 总结：为什么“注意力”就够了？ 传统方法：依赖复杂的循环结构，像用望远镜逐字查看 Transformer：用注意力机制直接建立全局联系，像展开整本书同时阅读 ","date":"2025-04-26T00:00:00Z","permalink":"http://localhost:1313/p/%E4%BB%8E%E9%9B%B6%E7%90%86%E8%A7%A3-transformer/","title":"从零理解 Transformer"},{"content":"1 问这个问题，正如问旅人为什么要出发一样，更多的是对未来的规划和当下身上包袱的剖析。虽然至今已经接触过数种语言，但是没有能够证明实力的项目，其次对于算法少有练习，同时又妄图涉足人工智能这个庞大的技术池。\n2 接下来的一周，我计划针对项目开发，算法练习，人工智能技术的掌握三方面努力，用实际行动检验自身漏洞。\n3 关于建站规划，我目前任未完全掌握配置stack主题种种细节，希望未来能够做用到简约的风格，记录前进路上的种种事件。\n（待续）\n引用 思念是最暖的忧伤像一双翅膀\n让我停不了飞不远在过往游荡\n不告而别的你 就算为了我着想\n这么沉痛的呵护 我怎么能翱翔\n最暖的憂傷 - 田馥甄\n","date":"2025-04-20T00:00:00Z","image":"http://localhost:1313/p/test/An%C2%A0Old%C2%A0Song-MoreanP.png","permalink":"http://localhost:1313/p/test/","title":"为什么创建博客（Ⅰ）"},{"content":"摘要 本文主要就个人博客建站思路做出讨论与梳理，同时根据当下现状为未来博客运营与发展方式寻找方向。\n绪论 个人博客历经从在线日记到知识共享平台的演变，已成为信息传播与个人品牌塑造的重要载体。截至2024年，全球独立博客站点超6亿个。国内研究聚焦内容生态构建，而国外则更关注架构创新。\n个人创建博客初衷主要基于： 1.对个人技术的提升与锻炼 2.对日常学习的记录与复盘 3.增加学习的乐趣与动力 Hugo、Git与Go语言的理论基础 Hugo技术特性 Hugo作为Go语言实现的静态网站生成器，其核心优势包括：\n高效编译：单二进制文件部署，10万节点Markdown文件生成耗时仅需毫秒级6； 模块化架构：通过themes目录支持主题热插拔，配合content与archetypes实现内容模板化39； 多环境适配：支持GitHub Pages自动化部署，降低服务器运维成本6。 Git版本控制 Git在博客开发中实现：\n协作管理：通过分支机制支持多作者协同编辑（如团队博客的git flow模型9）； 历史追溯：利用git log回滚错误内容修改，保障内容安全3。 Go语言底层支撑 Go语言的并发模型（Goroutine）使Hugo具备：\n高并发渲染：支持多核CPU并行生成页面，较Jekyll提速5倍6； 内存安全：垃圾回收机制避免内存泄漏，保障长期运行的稳定性9。 个人博客的发展现状分析 就所见而言，个人博客发展趋于成熟，各种优秀博客层出不穷，针对于技术方面的记录与教学十分宝贵，基于日常文字的表达更加真诚。\n结论 本研究验证了Hugo在降低建站成本、提升内容交付效率方面的技术优势，同时未来博客运营提出思考，争取实现可持续长久发展。\n致谢 感谢hugo生成器的支持和GitHub开源社区的技术文档贡献，以及Vercel平台提供部署的帮助。特别鸣谢Trrw为建站提供的巨大贡献，以及所有亲友的宝贵建议与鼓励。\n","date":"2025-04-18T00:00:00Z","image":"http://localhost:1313/p/test-chinese/DSC02865.jpg","permalink":"http://localhost:1313/p/test-chinese/","title":"关于建立个人博客的研究与思考"}]