<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Transformer on 满仓的博客</title>
        <link>http://localhost:1313/tags/transformer/</link>
        <description>Recent content in Transformer on 满仓的博客</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <lastBuildDate>Sat, 26 Apr 2025 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/tags/transformer/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>从零理解 Transformer</title>
        <link>http://localhost:1313/p/%E4%BB%8E%E9%9B%B6%E7%90%86%E8%A7%A3-transformer/</link>
        <pubDate>Sat, 26 Apr 2025 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/p/%E4%BB%8E%E9%9B%B6%E7%90%86%E8%A7%A3-transformer/</guid>
        <description>&lt;h1 id=&#34;从零理解-transformer为什么注意力改变了ai&#34;&gt;从零理解 Transformer：为什么“注意力”改变了AI？
&lt;/h1&gt;&lt;h2 id=&#34;人类如何处理信息-从直觉理解注意力&#34;&gt;人类如何处理信息？—— 从直觉理解“注意力”
&lt;/h2&gt;&lt;p&gt;假设你读这句话：&lt;strong&gt;“那只猫坐在垫子上，因为它很柔软。”&lt;/strong&gt;&lt;br&gt;
你会自然关注：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;“它”&lt;/strong&gt; 指代的是 &lt;strong&gt;“垫子”&lt;/strong&gt;（而不是猫）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;“柔软”&lt;/strong&gt; 是垫子的属性&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;关键直觉&lt;/strong&gt;：人类在处理信息时，会动态关注不同部分的关联性。这正是 Transformer 的核心思想！&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;传统模型的缺陷rnn-为什么不够好&#34;&gt;传统模型的缺陷：RNN 为什么不够好？
&lt;/h2&gt;&lt;h3 id=&#34;rnn-的工作原理&#34;&gt;RNN 的工作原理
&lt;/h3&gt;&lt;p&gt;传统模型（如RNN）像一条“传送带”，逐个单词处理：
句子：The → cat → sat → on → the → mat
处理：RNN逐步更新隐藏状态，最后输出结果&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;问题&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;处理到句尾时，可能已经忘记句首的信息&lt;/li&gt;
&lt;li&gt;无法并行计算（必须按顺序处理）&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;长距离依赖问题&#34;&gt;长距离依赖问题
&lt;/h3&gt;&lt;p&gt;例子：翻译 &lt;strong&gt;“The animal didn&amp;rsquo;t cross the street because it was too tired”&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RNN 可能混淆 &lt;strong&gt;“it”&lt;/strong&gt; 指代的是 &lt;strong&gt;“animal”&lt;/strong&gt; 还是 &lt;strong&gt;“street”&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;而人类能直接通过&lt;strong&gt;注意力&lt;/strong&gt;找到关联&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;transformer-的核心创新自注意力机制&#34;&gt;Transformer 的核心创新：自注意力机制
&lt;/h2&gt;&lt;h3 id=&#34;自注意力如何工作&#34;&gt;自注意力如何工作？
&lt;/h3&gt;&lt;p&gt;想象你在阅读句子**&amp;ldquo;猫坐在垫子上&amp;rdquo;**时，大脑会本能地：&lt;/p&gt;
&lt;p&gt;1️⃣ &lt;strong&gt;提取核心对象&lt;/strong&gt;（&amp;ldquo;猫&amp;quot;是主体）&lt;br&gt;
2️⃣ &lt;strong&gt;关联动作描述&lt;/strong&gt;（&amp;ldquo;坐&amp;quot;定义状态）&lt;br&gt;
3️⃣ &lt;strong&gt;忽略次要成分&lt;/strong&gt;（&amp;ldquo;上&amp;quot;仅表方位）&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-自注意力工作流程分解以猫为例&#34;&gt;🛠️ 自注意力工作流程分解（以&amp;quot;猫&amp;quot;为例）
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;步骤1：构建&amp;quot;问题-答案&amp;quot;映射系统&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;每个词语生成三组向量：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;向量类型&lt;/th&gt;
          &lt;th&gt;类比意义&lt;/th&gt;
          &lt;th&gt;实例说明&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Query&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&amp;ldquo;我想知道什么？&amp;rdquo;&lt;/td&gt;
          &lt;td&gt;&amp;ldquo;猫&amp;quot;需要知道自己的状态特征&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Key&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&amp;ldquo;我能回答什么？&amp;rdquo;&lt;/td&gt;
          &lt;td&gt;&amp;ldquo;坐&amp;quot;能提供动作信息&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Value&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&amp;ldquo;我的完整语义是什么？&amp;rdquo;&lt;/td&gt;
          &lt;td&gt;&amp;ldquo;垫子&amp;quot;包含物体属性信息&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;步骤2：建立语义关联度矩阵&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;通过点积计算&amp;quot;猫&amp;quot;与其他词语的关联强度：
&lt;/p&gt;
$$ \text{相似度}(Q_{\text{猫}}, K_{\text{坐}}) = \frac{Q_{\text{猫}} \cdot K_{\text{坐}}^T}{\sqrt{d_k}} $$&lt;p&gt;注意力权重分布&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;步骤3：信息融合与特征增强&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;加权聚合各词语的Value信息：&lt;/p&gt;
$$ \text{新表示}_{猫} = 0.6 \times V_{猫} + 0.3 \times V_{坐} + 0.1 \times V_{垫子} $$&lt;p&gt;
此时&amp;quot;猫&amp;quot;的表示包含：&lt;/p&gt;
&lt;p&gt;自身物种特征（60%）&lt;/p&gt;
&lt;p&gt;坐姿状态信息（30%）&lt;/p&gt;
&lt;p&gt;所处环境线索（10%）&lt;/p&gt;
&lt;h3 id=&#34;为什么要多头注意力-分工合作&#34;&gt;为什么要“多头”注意力？—— 分工合作
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;头1&lt;/strong&gt;：关注“猫”和“坐”的&lt;strong&gt;动作关系&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;头2&lt;/strong&gt;：关注“垫子”和“上”的&lt;strong&gt;位置关系&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;头3&lt;/strong&gt;：捕捉其他语法特征&lt;br&gt;
这样，每个头都能聚焦不同的信息，&lt;strong&gt;提高模型的表达能力&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;如何实现多头注意力-矩阵运算&#34;&gt;如何实现“多头”注意力？—— 矩阵运算
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;矩阵乘法&lt;/strong&gt;：计算注意力权重&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;点积&lt;/strong&gt;：衡量相似度&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Softmax&lt;/strong&gt;：归一化权重&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;transformer-的完整架构&#34;&gt;Transformer 的完整架构
&lt;/h2&gt;&lt;h3 id=&#34;两大核心模块&#34;&gt;两大核心模块
&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;  编码器（理解输入）                解码器（生成输出）
      ↓                               ↓
[自注意力 + 前馈网络]        [自注意力 + 交叉注意力 + 前馈网络]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;编码器详解&#34;&gt;编码器详解
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;输入嵌入&lt;/strong&gt;：将单词转换为向量
&lt;ul&gt;
&lt;li&gt;例：“猫” → [0.2, 1.3, -0.5, &amp;hellip;]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;位置编码&lt;/strong&gt;：添加位置信息（否则模型不知道单词顺序）
&lt;ul&gt;
&lt;li&gt;类似给每个词加“页码”：位置1 → 正弦波，位置2 → 不同正弦波&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自注意力层&lt;/strong&gt;：计算词与词之间的关系&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;前馈网络&lt;/strong&gt;：对每个词独立做非线性变换&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;解码器如何工作&#34;&gt;解码器如何工作？
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;输入&lt;/strong&gt;：已生成的部分结果（例如“猫 坐在”）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;掩码自注意力&lt;/strong&gt;：防止偷看未来信息（只能看已生成的词）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;交叉注意力&lt;/strong&gt;：连接编码器的输出（知道原文信息）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;预测下一个词&lt;/strong&gt;：输出概率分布（例如“垫子”的概率最高）&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;为什么-transformer-如此强大&#34;&gt;为什么 Transformer 如此强大？
&lt;/h2&gt;&lt;h3 id=&#34;三大优势&#34;&gt;三大优势
&lt;/h3&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;优势&lt;/th&gt;
          &lt;th&gt;说明&lt;/th&gt;
          &lt;th&gt;类比&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;并行计算&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;同时处理所有单词，速度极快&lt;/td&gt;
          &lt;td&gt;多人协同工作&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;全局视野&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;任意两个词可直接交互，无论距离多远&lt;/td&gt;
          &lt;td&gt;直接查阅全书任意章节&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;灵活扩展&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;通过堆叠更多层提升性能&lt;/td&gt;
          &lt;td&gt;增加专家团队规模&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;实际影响&#34;&gt;实际影响
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;BERT&lt;/strong&gt;：理解语言含义的模型（用于搜索引擎、问答系统）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GPT&lt;/strong&gt;：生成文本的模型（用于写作助手、聊天机器人）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DALL·E&lt;/strong&gt;：生成图像的模型也借鉴了注意力机制&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;关键数学公式&#34;&gt;关键数学公式
&lt;/h2&gt;&lt;h3 id=&#34;自注意力公式&#34;&gt;自注意力公式
&lt;/h3&gt;$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\dfrac{QK^T}{\sqrt{d_k}}\right)V
$$&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Q&lt;/code&gt;（查询）：当前关注的焦点&lt;/li&gt;
&lt;li&gt;&lt;code&gt;K&lt;/code&gt;（键）：其他词的身份标识&lt;/li&gt;
&lt;li&gt;&lt;code&gt;V&lt;/code&gt;（值）：实际携带的信息&lt;/li&gt;
&lt;li&gt;&lt;code&gt;d_k&lt;/code&gt;：缩放因子，防止数值过大&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;位置编码公式&#34;&gt;位置编码公式
&lt;/h3&gt;$$
PE_{(pos,2i)} = \sin\left(pos/10000^{2i/d}\right) \\  
PE_{(pos,2i+1)} = \cos\left(pos/10000^{2i/d}\right)
$$&lt;ul&gt;
&lt;li&gt;通过正弦波组合，让模型感知位置差异&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;多头注意力公式&#34;&gt;多头注意力公式
&lt;/h3&gt;$$
\text{MultiHead}(Q, K, V) = \text{Concat}\left(\text{head}_1, \dots, \text{head}_h\right)W^O
$$&lt;ul&gt;
&lt;li&gt;多个头并行计算，最后合并结果&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;前馈网络公式&#34;&gt;前馈网络公式
&lt;/h3&gt;$$
\text{FFN}(x) = \text{max}(0, xW_1 + b_1)W_2 + b_2
$$&lt;ul&gt;
&lt;li&gt;两层线性变换，增加非线性表达能力&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;损失函数公式&#34;&gt;损失函数公式
&lt;/h3&gt;$$
\text{Loss} = -\sum_{i=1}^{N}\log p(y_i|x)
$$&lt;ul&gt;
&lt;li&gt;交叉熵损失，用于训练模型&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;梯度下降公式&#34;&gt;梯度下降公式
&lt;/h3&gt;$$
\theta_{t+1} = \theta_t - \alpha \nabla_{\theta} \text{Loss}
$$&lt;ul&gt;
&lt;li&gt;反向传播更新模型参数&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;总结为什么注意力就够了&#34;&gt;总结：为什么“注意力”就够了？
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;传统方法&lt;/strong&gt;：依赖复杂的循环结构，像用望远镜逐字查看&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Transformer&lt;/strong&gt;：用注意力机制直接建立全局联系，像展开整本书同时阅读&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        
    </channel>
</rss>
