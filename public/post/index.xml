<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on 满仓的博客</title>
        <link>http://localhost:1313/post/</link>
        <description>Recent content in Posts on 满仓的博客</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>zh-cn</language>
        <lastBuildDate>Sat, 26 Apr 2025 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:1313/post/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>从零理解 Transformer</title>
        <link>http://localhost:1313/p/%E4%BB%8E%E9%9B%B6%E7%90%86%E8%A7%A3-transformer/</link>
        <pubDate>Sat, 26 Apr 2025 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/p/%E4%BB%8E%E9%9B%B6%E7%90%86%E8%A7%A3-transformer/</guid>
        <description>&lt;h1 id=&#34;从零理解-transformer为什么注意力改变了ai&#34;&gt;从零理解 Transformer：为什么“注意力”改变了AI？
&lt;/h1&gt;&lt;h2 id=&#34;人类如何处理信息-从直觉理解注意力&#34;&gt;人类如何处理信息？—— 从直觉理解“注意力”
&lt;/h2&gt;&lt;p&gt;假设你读这句话：&lt;strong&gt;“那只猫坐在垫子上，因为它很柔软。”&lt;/strong&gt;&lt;br&gt;
你会自然关注：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;“它”&lt;/strong&gt; 指代的是 &lt;strong&gt;“垫子”&lt;/strong&gt;（而不是猫）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;“柔软”&lt;/strong&gt; 是垫子的属性&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;关键直觉&lt;/strong&gt;：人类在处理信息时，会动态关注不同部分的关联性。这正是 Transformer 的核心思想！&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;传统模型的缺陷rnn-为什么不够好&#34;&gt;传统模型的缺陷：RNN 为什么不够好？
&lt;/h2&gt;&lt;h3 id=&#34;rnn-的工作原理&#34;&gt;RNN 的工作原理
&lt;/h3&gt;&lt;p&gt;传统模型（如RNN）像一条“传送带”，逐个单词处理：
句子：The → cat → sat → on → the → mat
处理：RNN逐步更新隐藏状态，最后输出结果&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;问题&lt;/strong&gt;：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;处理到句尾时，可能已经忘记句首的信息&lt;/li&gt;
&lt;li&gt;无法并行计算（必须按顺序处理）&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;长距离依赖问题&#34;&gt;长距离依赖问题
&lt;/h3&gt;&lt;p&gt;例子：翻译 &lt;strong&gt;“The animal didn&amp;rsquo;t cross the street because it was too tired”&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RNN 可能混淆 &lt;strong&gt;“it”&lt;/strong&gt; 指代的是 &lt;strong&gt;“animal”&lt;/strong&gt; 还是 &lt;strong&gt;“street”&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;而人类能直接通过&lt;strong&gt;注意力&lt;/strong&gt;找到关联&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;transformer-的核心创新自注意力机制&#34;&gt;Transformer 的核心创新：自注意力机制
&lt;/h2&gt;&lt;h3 id=&#34;自注意力如何工作&#34;&gt;自注意力如何工作？
&lt;/h3&gt;&lt;p&gt;想象你在阅读句子**&amp;ldquo;猫坐在垫子上&amp;rdquo;**时，大脑会本能地：&lt;/p&gt;
&lt;p&gt;1️⃣ &lt;strong&gt;提取核心对象&lt;/strong&gt;（&amp;ldquo;猫&amp;quot;是主体）&lt;br&gt;
2️⃣ &lt;strong&gt;关联动作描述&lt;/strong&gt;（&amp;ldquo;坐&amp;quot;定义状态）&lt;br&gt;
3️⃣ &lt;strong&gt;忽略次要成分&lt;/strong&gt;（&amp;ldquo;上&amp;quot;仅表方位）&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;-自注意力工作流程分解以猫为例&#34;&gt;🛠️ 自注意力工作流程分解（以&amp;quot;猫&amp;quot;为例）
&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;步骤1：构建&amp;quot;问题-答案&amp;quot;映射系统&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;每个词语生成三组向量：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;向量类型&lt;/th&gt;
          &lt;th&gt;类比意义&lt;/th&gt;
          &lt;th&gt;实例说明&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Query&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&amp;ldquo;我想知道什么？&amp;rdquo;&lt;/td&gt;
          &lt;td&gt;&amp;ldquo;猫&amp;quot;需要知道自己的状态特征&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Key&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&amp;ldquo;我能回答什么？&amp;rdquo;&lt;/td&gt;
          &lt;td&gt;&amp;ldquo;坐&amp;quot;能提供动作信息&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;Value&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;&amp;ldquo;我的完整语义是什么？&amp;rdquo;&lt;/td&gt;
          &lt;td&gt;&amp;ldquo;垫子&amp;quot;包含物体属性信息&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;步骤2：建立语义关联度矩阵&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;通过点积计算&amp;quot;猫&amp;quot;与其他词语的关联强度：
&lt;/p&gt;
$$ \text{相似度}(Q_{\text{猫}}, K_{\text{坐}}) = \frac{Q_{\text{猫}} \cdot K_{\text{坐}}^T}{\sqrt{d_k}} $$&lt;p&gt;注意力权重分布&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;步骤3：信息融合与特征增强&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;加权聚合各词语的Value信息：&lt;/p&gt;
$$ \text{新表示}_{猫} = 0.6 \times V_{猫} + 0.3 \times V_{坐} + 0.1 \times V_{垫子} $$&lt;p&gt;
此时&amp;quot;猫&amp;quot;的表示包含：&lt;/p&gt;
&lt;p&gt;自身物种特征（60%）&lt;/p&gt;
&lt;p&gt;坐姿状态信息（30%）&lt;/p&gt;
&lt;p&gt;所处环境线索（10%）&lt;/p&gt;
&lt;h3 id=&#34;为什么要多头注意力-分工合作&#34;&gt;为什么要“多头”注意力？—— 分工合作
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;头1&lt;/strong&gt;：关注“猫”和“坐”的&lt;strong&gt;动作关系&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;头2&lt;/strong&gt;：关注“垫子”和“上”的&lt;strong&gt;位置关系&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;头3&lt;/strong&gt;：捕捉其他语法特征&lt;br&gt;
这样，每个头都能聚焦不同的信息，&lt;strong&gt;提高模型的表达能力&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;如何实现多头注意力-矩阵运算&#34;&gt;如何实现“多头”注意力？—— 矩阵运算
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;矩阵乘法&lt;/strong&gt;：计算注意力权重&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;点积&lt;/strong&gt;：衡量相似度&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Softmax&lt;/strong&gt;：归一化权重&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;transformer-的完整架构&#34;&gt;Transformer 的完整架构
&lt;/h2&gt;&lt;h3 id=&#34;两大核心模块&#34;&gt;两大核心模块
&lt;/h3&gt;&lt;pre&gt;&lt;code&gt;  编码器（理解输入）                解码器（生成输出）
      ↓                               ↓
[自注意力 + 前馈网络]        [自注意力 + 交叉注意力 + 前馈网络]
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;编码器详解&#34;&gt;编码器详解
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;输入嵌入&lt;/strong&gt;：将单词转换为向量
&lt;ul&gt;
&lt;li&gt;例：“猫” → [0.2, 1.3, -0.5, &amp;hellip;]&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;位置编码&lt;/strong&gt;：添加位置信息（否则模型不知道单词顺序）
&lt;ul&gt;
&lt;li&gt;类似给每个词加“页码”：位置1 → 正弦波，位置2 → 不同正弦波&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;自注意力层&lt;/strong&gt;：计算词与词之间的关系&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;前馈网络&lt;/strong&gt;：对每个词独立做非线性变换&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;解码器如何工作&#34;&gt;解码器如何工作？
&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;输入&lt;/strong&gt;：已生成的部分结果（例如“猫 坐在”）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;掩码自注意力&lt;/strong&gt;：防止偷看未来信息（只能看已生成的词）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;交叉注意力&lt;/strong&gt;：连接编码器的输出（知道原文信息）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;预测下一个词&lt;/strong&gt;：输出概率分布（例如“垫子”的概率最高）&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id=&#34;为什么-transformer-如此强大&#34;&gt;为什么 Transformer 如此强大？
&lt;/h2&gt;&lt;h3 id=&#34;三大优势&#34;&gt;三大优势
&lt;/h3&gt;&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;优势&lt;/th&gt;
          &lt;th&gt;说明&lt;/th&gt;
          &lt;th&gt;类比&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;并行计算&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;同时处理所有单词，速度极快&lt;/td&gt;
          &lt;td&gt;多人协同工作&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;全局视野&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;任意两个词可直接交互，无论距离多远&lt;/td&gt;
          &lt;td&gt;直接查阅全书任意章节&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;strong&gt;灵活扩展&lt;/strong&gt;&lt;/td&gt;
          &lt;td&gt;通过堆叠更多层提升性能&lt;/td&gt;
          &lt;td&gt;增加专家团队规模&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;实际影响&#34;&gt;实际影响
&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;BERT&lt;/strong&gt;：理解语言含义的模型（用于搜索引擎、问答系统）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;GPT&lt;/strong&gt;：生成文本的模型（用于写作助手、聊天机器人）&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;DALL·E&lt;/strong&gt;：生成图像的模型也借鉴了注意力机制&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;关键数学公式&#34;&gt;关键数学公式
&lt;/h2&gt;&lt;h3 id=&#34;自注意力公式&#34;&gt;自注意力公式
&lt;/h3&gt;$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\dfrac{QK^T}{\sqrt{d_k}}\right)V
$$&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Q&lt;/code&gt;（查询）：当前关注的焦点&lt;/li&gt;
&lt;li&gt;&lt;code&gt;K&lt;/code&gt;（键）：其他词的身份标识&lt;/li&gt;
&lt;li&gt;&lt;code&gt;V&lt;/code&gt;（值）：实际携带的信息&lt;/li&gt;
&lt;li&gt;&lt;code&gt;d_k&lt;/code&gt;：缩放因子，防止数值过大&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;位置编码公式&#34;&gt;位置编码公式
&lt;/h3&gt;$$
PE_{(pos,2i)} = \sin\left(pos/10000^{2i/d}\right) \\  
PE_{(pos,2i+1)} = \cos\left(pos/10000^{2i/d}\right)
$$&lt;ul&gt;
&lt;li&gt;通过正弦波组合，让模型感知位置差异&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;多头注意力公式&#34;&gt;多头注意力公式
&lt;/h3&gt;$$
\text{MultiHead}(Q, K, V) = \text{Concat}\left(\text{head}_1, \dots, \text{head}_h\right)W^O
$$&lt;ul&gt;
&lt;li&gt;多个头并行计算，最后合并结果&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;前馈网络公式&#34;&gt;前馈网络公式
&lt;/h3&gt;$$
\text{FFN}(x) = \text{max}(0, xW_1 + b_1)W_2 + b_2
$$&lt;ul&gt;
&lt;li&gt;两层线性变换，增加非线性表达能力&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;损失函数公式&#34;&gt;损失函数公式
&lt;/h3&gt;$$
\text{Loss} = -\sum_{i=1}^{N}\log p(y_i|x)
$$&lt;ul&gt;
&lt;li&gt;交叉熵损失，用于训练模型&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;梯度下降公式&#34;&gt;梯度下降公式
&lt;/h3&gt;$$
\theta_{t+1} = \theta_t - \alpha \nabla_{\theta} \text{Loss}
$$&lt;ul&gt;
&lt;li&gt;反向传播更新模型参数&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;总结为什么注意力就够了&#34;&gt;总结：为什么“注意力”就够了？
&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;传统方法&lt;/strong&gt;：依赖复杂的循环结构，像用望远镜逐字查看&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Transformer&lt;/strong&gt;：用注意力机制直接建立全局联系，像展开整本书同时阅读&lt;/li&gt;
&lt;/ul&gt;
</description>
        </item>
        <item>
        <title>为什么创建博客（Ⅰ）</title>
        <link>http://localhost:1313/p/test/</link>
        <pubDate>Sun, 20 Apr 2025 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/p/test/</guid>
        <description>&lt;img src="http://localhost:1313/p/test/An%C2%A0Old%C2%A0Song-MoreanP.png" alt="Featured image of post 为什么创建博客（Ⅰ）" /&gt;&lt;h2 id=&#34;1&#34;&gt;1
&lt;/h2&gt;&lt;p&gt;问这个问题，正如问旅人为什么要出发一样，更多的是对未来的规划和当下身上包袱的剖析。虽然至今已经接触过数种语言，但是没有能够证明实力的项目，其次对于算法少有练习，同时又妄图涉足人工智能这个庞大的技术池。&lt;/p&gt;
&lt;h2 id=&#34;2&#34;&gt;2
&lt;/h2&gt;&lt;p&gt;接下来的一周，我计划针对项目开发，算法练习，人工智能技术的掌握三方面努力，用实际行动检验自身漏洞。&lt;/p&gt;
&lt;h2 id=&#34;3&#34;&gt;3
&lt;/h2&gt;&lt;p&gt;关于建站规划，我目前任未完全掌握配置stack主题种种细节，希望未来能够做用到简约的风格，记录前进路上的种种事件。&lt;/p&gt;
&lt;p&gt;（待续）&lt;/p&gt;
&lt;h2 id=&#34;引用&#34;&gt;引用
&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;思念是最暖的忧伤像一双翅膀&lt;br&gt;
让我停不了飞不远在过往游荡&lt;br&gt;
不告而别的你 就算为了我着想&lt;br&gt;
这么沉痛的呵护 我怎么能翱翔&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;a class=&#34;link&#34; href=&#34;https://www.youtube.com/watch?v=3aypp_YlBzI&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;最暖的憂傷 - 田馥甄&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;&lt;/blockquote&gt;
</description>
        </item>
        <item>
        <title>关于建立个人博客的研究与思考</title>
        <link>http://localhost:1313/p/test-chinese/</link>
        <pubDate>Fri, 18 Apr 2025 00:00:00 +0000</pubDate>
        
        <guid>http://localhost:1313/p/test-chinese/</guid>
        <description>&lt;img src="http://localhost:1313/p/test-chinese/DSC02865.jpg" alt="Featured image of post 关于建立个人博客的研究与思考" /&gt;&lt;h3 id=&#34;摘要&#34;&gt;摘要
&lt;/h3&gt;&lt;p&gt;本文主要就个人博客建站思路做出讨论与梳理，同时根据当下现状为未来博客运营与发展方式寻找方向。&lt;/p&gt;
&lt;h3 id=&#34;绪论&#34;&gt;绪论
&lt;/h3&gt;&lt;p&gt;个人博客历经从在线日记到知识共享平台的演变，已成为信息传播与个人品牌塑造的重要载体。截至2024年，全球独立博客站点超6亿个。国内研究聚焦内容生态构建，而国外则更关注架构创新。&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    个人创建博客初衷主要基于：

        1.对个人技术的提升与锻炼 

        2.对日常学习的记录与复盘 

        3.增加学习的乐趣与动力
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;hugogit与go语言的理论基础&#34;&gt;Hugo、Git与Go语言的理论基础
&lt;/h3&gt;&lt;h4 id=&#34;hugo技术特性&#34;&gt;Hugo技术特性
&lt;/h4&gt;&lt;p&gt;Hugo作为Go语言实现的静态网站生成器，其核心优势包括：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;   高效编译：单二进制文件部署，10万节点Markdown文件生成耗时仅需毫秒级6；

   模块化架构：通过themes目录支持主题热插拔，配合content与archetypes实现内容模板化39；

   多环境适配：支持GitHub Pages自动化部署，降低服务器运维成本6。
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;git版本控制&#34;&gt;Git版本控制
&lt;/h4&gt;&lt;p&gt;Git在博客开发中实现：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;   协作管理：通过分支机制支持多作者协同编辑（如团队博客的git flow模型9）；

   历史追溯：利用git log回滚错误内容修改，保障内容安全3。
&lt;/code&gt;&lt;/pre&gt;
&lt;h4 id=&#34;go语言底层支撑&#34;&gt;Go语言底层支撑
&lt;/h4&gt;&lt;p&gt;Go语言的并发模型（Goroutine）使Hugo具备：&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    高并发渲染：支持多核CPU并行生成页面，较Jekyll提速5倍6；

    内存安全：垃圾回收机制避免内存泄漏，保障长期运行的稳定性9。
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;个人博客的发展现状分析&#34;&gt;个人博客的发展现状分析
&lt;/h3&gt;&lt;p&gt;就所见而言，个人博客发展趋于成熟，各种优秀博客层出不穷，针对于技术方面的记录与教学十分宝贵，基于日常文字的表达更加真诚。&lt;/p&gt;
&lt;h3 id=&#34;结论&#34;&gt;结论
&lt;/h3&gt;&lt;p&gt;本研究验证了Hugo在降低建站成本、提升内容交付效率方面的技术优势，同时未来博客运营提出思考，争取实现可持续长久发展。&lt;/p&gt;
&lt;h3 id=&#34;致谢&#34;&gt;致谢
&lt;/h3&gt;&lt;p&gt;感谢hugo生成器的支持和GitHub开源社区的技术文档贡献，以及Vercel平台提供部署的帮助。特别鸣谢Trrw为建站提供的巨大贡献，以及所有亲友的宝贵建议与鼓励。&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
